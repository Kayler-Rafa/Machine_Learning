# -*- coding: utf-8 -*-
"""DataSetA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LPaNFcFmMNeTYIhA4zyvW475Ayu39dXH
"""

import pandas as pd
import numpy as np

df = pd.read_csv('/content/data.csv')

df.head()

missing_values = df.isnull().mean() * 100

missing_values[missing_values > 30]

columns_to_keep = missing_values[missing_values <= 30].index
df = df[columns_to_keep]

import seaborn as sns
import matplotlib.pyplot as plt

correlation = df.corr()['Bankrupt?'].drop('Bankrupt?')

print("Top 10 variáveis com maior correlação positiva:")
print(correlation.sort_values(ascending=False).head(10))

print("\nTop 10 variáveis com maior correlação negativa:")
print(correlation.sort_values().head(10))

plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(), cmap='coolwarm', center=0)
plt.title('Matriz de Correlação')
plt.show()

relevant_features = correlation[correlation.abs() > 0.1].index.tolist()

relevant_features.append('Bankrupt?')

df_selected = df[relevant_features]

from sklearn.preprocessing import StandardScaler

X = df_selected.drop('Bankrupt?', axis=1)
y = df_selected['Bankrupt?']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

X_scaled_df.head()

from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import seaborn as sns

sns.set(style="whitegrid")

scaler = MinMaxScaler()
X_normalized = scaler.fit_transform(X)

X_normalized_df = pd.DataFrame(X_normalized, columns=X.columns)

X_normalized_df.hist(bins=20, figsize=(15, 10))
plt.suptitle("Distribuição das Features Normalizadas (Grupo A)")
plt.show()

from sklearn.preprocessing import OneHotEncoder

categorical_cols = df_selected.select_dtypes(include=['object', 'category']).columns.tolist()

if categorical_cols:
    encoder = OneHotEncoder(drop='first', sparse=False, handle_unknown='ignore')
    encoded = encoder.fit_transform(df_selected[categorical_cols])
    encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(categorical_cols), index=df_selected.index)

    df_encoded = pd.concat([df_selected.drop(categorical_cols, axis=1), encoded_df], axis=1)
else:
    df_encoded = df_selected.copy()

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

X = df_encoded.drop('Bankrupt?', axis=1)
y = df_encoded['Bankrupt?']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
pca_df['Bankrupt?'] = y.values

import matplotlib.pyplot as plt
import seaborn as sns

sns.set(style="whitegrid")

plt.figure(figsize=(10, 7))
sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='Bankrupt?', palette='Set1')
plt.title('PCA - Componentes Principais')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.legend(title='Bankrupt?')
plt.show()

from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
import matplotlib.pyplot as plt
import seaborn as sns

models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'KNN': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(),
    'Naive Bayes': GaussianNB(),
    'Neural Network': MLPClassifier(max_iter=1000)
}

params = {
    'Logistic Regression': {'C': [0.01, 0.1, 1, 10]},
    'KNN': {'n_neighbors': [3, 5, 7]},
    'Decision Tree': {'max_depth': [3, 5, 10]},
    'Naive Bayes': {},
    'Neural Network': {'hidden_layer_sizes': [(50,), (100,)], 'alpha': [0.0001, 0.001]}
}

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

results = []

for name, model in models.items():
    print(f"Treinando: {name}")

    clf = GridSearchCV(model, params[name], cv=skf, scoring='accuracy', n_jobs=-1) if params[name] else model
    if params[name]:
        clf.fit(X_normalized, y)
        best_model = clf.best_estimator_
    else:
        best_model = model.fit(X_normalized, y)

    accuracies, precisions, recalls, f1s, aucs = [], [], [], [], []

    for train_index, test_index in skf.split(X_normalized, y):
        X_train, X_test = X_normalized[train_index], X_normalized[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        best_model.fit(X_train, y_train)
        y_pred = best_model.predict(X_test)
        y_prob = best_model.predict_proba(X_test)[:,1] if hasattr(best_model, "predict_proba") else None

        accuracies.append(accuracy_score(y_test, y_pred))
        precisions.append(precision_score(y_test, y_pred))
        recalls.append(recall_score(y_test, y_pred))
        f1s.append(f1_score(y_test, y_pred))
        if y_prob is not None:
            aucs.append(roc_auc_score(y_test, y_prob))

    print(f"Resultados para {name}:")
    print(f"Acurácia média: {np.mean(accuracies):.4f}")
    print(f"Precisão média: {np.mean(precisions):.4f}")
    print(f"Recall médio: {np.mean(recalls):.4f}")
    print(f"F1-score médio: {np.mean(f1s):.4f}")
    if aucs:
        print(f"AUC-ROC média: {np.mean(aucs):.4f}")

    results.append({
        'Modelo': name,
        'Acurácia': np.mean(accuracies),
        'Precisão': np.mean(precisions),
        'Recall': np.mean(recalls),
        'F1': np.mean(f1s),
        'AUC': np.mean(aucs) if aucs else None
    })

    X_train, X_test = X_normalized[train_index], X_normalized[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    best_model.fit(X_train, y_train)
    y_pred = best_model.predict(X_test)
    y_prob = best_model.predict_proba(X_test)[:,1] if hasattr(best_model, "predict_proba") else None

    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f"Matriz de Confusão - {name}")
    plt.xlabel("Predito")
    plt.ylabel("Verdadeiro")
    plt.show()

    if y_prob is not None:
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlabel('Falso Positivo')
        plt.ylabel('Verdadeiro Positivo')
        plt.title(f'Curva ROC - {name}')
        plt.legend(loc='lower right')
        plt.show()

from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score, confusion_matrix
from sklearn.decomposition import PCA
import seaborn as sns
import matplotlib.pyplot as plt

n_clusters = len(set(y))

kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X_normalized)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_normalized)

plt.figure(figsize=(10, 7))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=clusters, palette='Set2')
plt.title("Clusters identificados pelo K-Means (via PCA)")
plt.xlabel("Componente Principal 1")
plt.ylabel("Componente Principal 2")
plt.show()

cluster_means = pd.DataFrame(X_normalized, columns=X.columns)
cluster_means['Cluster'] = clusters
means_by_cluster = cluster_means.groupby('Cluster').mean()
print("Médias das features por cluster:")
display(means_by_cluster)

ari = adjusted_rand_score(y, clusters)
print(f"Adjusted Rand Index (ARI): {ari:.4f}")

cm = confusion_matrix(y, clusters)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Matriz de Confusão: Classes Reais vs Clusters")
plt.xlabel("Clusters")
plt.ylabel("Classes Reais")
plt.show()