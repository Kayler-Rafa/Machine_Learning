# -*- coding: utf-8 -*-
"""DataSetB

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19zxWE8z_cF3Cq5Uccug1F-rv4zj4o57M
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

file_path = '/content/MiniBooNE_PID.txt'

with open(file_path, 'r') as file:
    first_line = file.readline()
    signal_count, background_count = map(int, first_line.strip().split())

total_samples = signal_count + background_count

data = pd.read_csv(file_path, skiprows=1, delim_whitespace=True, header=None)

assert data.shape[0] == total_samples, "Número de amostras não corresponde ao especificado na primeira linha."

labels = np.array([1]*signal_count + [0]*background_count)

missing_values = data.isnull().sum().sum()
print(f'Total de valores ausentes: {missing_values}')

scaler = MinMaxScaler()
data_normalized = scaler.fit_transform(data)

data_normalized_df = pd.DataFrame(data_normalized, columns=[f'feature_{i+1}' for i in range(data.shape[1])])

data_normalized_df.head()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(data_normalized_df)

plt.figure(figsize=(10, 7))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=labels, palette='Set1', alpha=0.5)
plt.title('Projeção PCA (2 Componentes Principais)')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.legend(title='Classe')
plt.show()

explained_variance = pca.explained_variance_ratio_
cumulative_variance = explained_variance.cumsum()

plt.figure(figsize=(10, 5))
plt.bar(range(1, 3), explained_variance, alpha=0.5, align='center', label='Variância Explicada Individual')
plt.step(range(1, 3), cumulative_variance, where='mid', label='Variância Explicada Acumulada')
plt.xlabel('Componentes Principais')
plt.ylabel('Proporção da Variância Explicada')
plt.title('Variância Explicada por PCA')
plt.legend(loc='best')
plt.tight_layout()
plt.show()

from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
import matplotlib.pyplot as plt
import seaborn as sns

models = {
    'Logistic Regression': (LogisticRegression(max_iter=1000), {'C': [0.01, 0.1, 1, 10]}),
    'KNN': (KNeighborsClassifier(), {'n_neighbors': [3, 5, 7]}),
    'Decision Tree': (DecisionTreeClassifier(), {'max_depth': [5, 10, 15]}),
    'Naive Bayes': (GaussianNB(), {}),
    'Neural Network': (MLPClassifier(max_iter=300), {'hidden_layer_sizes': [(50,), (100,)], 'alpha': [0.0001, 0.001]})
}

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

results = {}
for name, (model, params) in models.items():
    print(f"\nTreinando: {name}")
    best_model = None

    if params:
        grid = GridSearchCV(model, params, cv=skf, scoring='accuracy', n_jobs=-1)
        grid.fit(data_normalized, labels)
        best_model = grid.best_estimator_
    else:
        best_model = model
        best_model.fit(data_normalized, labels)

    accuracies, precisions, recalls, f1s, aucs = [], [], [], [], []

    for train_idx, test_idx in skf.split(data_normalized, labels):
        X_train, X_test = data_normalized[train_idx], data_normalized[test_idx]
        y_train, y_test = labels[train_idx], labels[test_idx]

        best_model.fit(X_train, y_train)
        y_pred = best_model.predict(X_test)
        y_proba = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, "predict_proba") else best_model.decision_function(X_test)

        accuracies.append(accuracy_score(y_test, y_pred))
        precisions.append(precision_score(y_test, y_pred))
        recalls.append(recall_score(y_test, y_pred))
        f1s.append(f1_score(y_test, y_pred))
        aucs.append(roc_auc_score(y_test, y_proba))

        cm = confusion_matrix(y_test, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title(f"Matriz de Confusão - {name}")
        plt.xlabel('Predito')
        plt.ylabel('Real')
        plt.show()

        fpr, tpr, thresholds = roc_curve(y_test, y_proba)
        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc_score(y_test, y_proba):.2f})')
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlabel('Falso Positivo')
        plt.ylabel('Verdadeiro Positivo')
        plt.title(f'Curva ROC - {name}')
        plt.legend()
        plt.show()

    print(f"Resultados para {name}:")
    print(f"Acurácia média: {np.mean(accuracies):.4f}")
    print(f"Precisão média: {np.mean(precisions):.4f}")
    print(f"Recall médio: {np.mean(recalls):.4f}")
    print(f"F1-score média: {np.mean(f1s):.4f}")
    print(f"AUC-ROC média: {np.mean(aucs):.4f}")

from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
import pandas as pd

n_clusters = len(set(labels))

kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
clusters = kmeans.fit_predict(data_normalized)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(data_normalized)

plt.figure(figsize=(10, 7))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=clusters, palette='Set2')
plt.title("Clusters identificados pelo K-Means (via PCA)")
plt.xlabel("Componente Principal 1")
plt.ylabel("Componente Principal 2")
plt.show()

cluster_means = pd.DataFrame(data_normalized, columns=[f'feature_{i+1}' for i in range(data.shape[1])])
cluster_means['Cluster'] = clusters
means_by_cluster = cluster_means.groupby('Cluster').mean()
print("Médias das features por cluster:")
display(means_by_cluster)

ari = adjusted_rand_score(labels, clusters)
print(f"Adjusted Rand Index (ARI): {ari:.4f}")

cm = confusion_matrix(labels, clusters)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Matriz de Confusão: Classes Reais vs Clusters")
plt.xlabel("Clusters")
plt.ylabel("Classes Reais")
plt.show()